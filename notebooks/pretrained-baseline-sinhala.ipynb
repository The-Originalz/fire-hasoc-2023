{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:54:53.110224Z","iopub.status.busy":"2023-08-25T20:54:53.109798Z","iopub.status.idle":"2023-08-25T20:55:19.953289Z","shell.execute_reply":"2023-08-25T20:55:19.952131Z","shell.execute_reply.started":"2023-08-25T20:54:53.110178Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/mrutyunjaybiswal/Documents/rcml/hasoc23-hate-speech-bengali-bodo-assamese/hasoc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import re\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorWithPadding\n","from transformers import create_optimizer\n","from transformers.keras_callbacks import KerasMetricCallback\n","from transformers import TFAutoModelForSequenceClassification\n","\n","from sklearn.metrics import classification_report, f1_score\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:19.956512Z","iopub.status.busy":"2023-08-25T20:55:19.956139Z","iopub.status.idle":"2023-08-25T20:55:20.011716Z","shell.execute_reply":"2023-08-25T20:55:20.010753Z","shell.execute_reply.started":"2023-08-25T20:55:19.956477Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"./../inp/sinhala/train.csv\")\n","test_data = pd.read_csv(\"./../inp/sinhala/test.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.085228Z","iopub.status.busy":"2023-08-25T20:55:20.084904Z","iopub.status.idle":"2023-08-25T20:55:20.089737Z","shell.execute_reply":"2023-08-25T20:55:20.088734Z","shell.execute_reply.started":"2023-08-25T20:55:20.085196Z"},"trusted":true},"outputs":[],"source":["id2label = {0: \"NOT\", 1: \"HOF\"}\n","label2id = {\"NOT\": 0, \"HOF\": 1}"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.092143Z","iopub.status.busy":"2023-08-25T20:55:20.091394Z","iopub.status.idle":"2023-08-25T20:55:20.100560Z","shell.execute_reply":"2023-08-25T20:55:20.099676Z","shell.execute_reply.started":"2023-08-25T20:55:20.092084Z"},"trusted":true},"outputs":[],"source":["def data_clean(data_df):\n","    # Removing Unwanted Columns\n","    data_df.drop([\"post_id\"], axis=1, inplace=True)\n","    \n","    # Removing @tags \n","    pattern = r'@\\w+'\n","    data_df[\"text\"] = data_df[\"text\"].apply(lambda x: re.sub(pattern, '', x))\n","    \n","    # Transforming Categorical Values to Numericals\n","    data_df[\"labels\"] = data_df[\"label\"].apply(lambda x: [label2id[x]])\n","    \n","    # Dropping label column\n","    data_df.drop(\"label\", axis=1, inplace=True)\n","    \n","    return data_df"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.104445Z","iopub.status.busy":"2023-08-25T20:55:20.104144Z","iopub.status.idle":"2023-08-25T20:55:20.123435Z","shell.execute_reply":"2023-08-25T20:55:20.122458Z","shell.execute_reply.started":"2023-08-25T20:55:20.104416Z"},"trusted":true},"outputs":[],"source":["# Cleaning and Preparing Test Data\n","test_id = test_data[\"post_id\"]\n","test_data.drop(\"post_id\", axis=1, inplace=True)\n","pattern = r'@\\w+'\n","test_data[\"text\"] = test_data[\"text\"].apply(lambda x: re.sub(pattern, '', x))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.125659Z","iopub.status.busy":"2023-08-25T20:55:20.125221Z","iopub.status.idle":"2023-08-25T20:55:21.347551Z","shell.execute_reply":"2023-08-25T20:55:21.346508Z","shell.execute_reply.started":"2023-08-25T20:55:20.125573Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading (…)lve/main/config.json: 100%|██████████| 551/551 [00:00<00:00, 783kB/s]\n","Downloading (…)olve/main/vocab.json: 100%|██████████| 1.08M/1.08M [00:00<00:00, 1.28MB/s]\n","Downloading (…)olve/main/merges.txt: 100%|██████████| 721k/721k [00:00<00:00, 819kB/s]\n"]}],"source":["model_name = \"keshan/SinhalaBERTo\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, from_pt=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.349436Z","iopub.status.busy":"2023-08-25T20:55:21.348943Z","iopub.status.idle":"2023-08-25T20:55:21.866335Z","shell.execute_reply":"2023-08-25T20:55:21.865190Z","shell.execute_reply.started":"2023-08-25T20:55:21.349391Z"},"trusted":true},"outputs":[],"source":["def tokenize_examples(examples):\n","    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, max_length=True)\n","    return tokenized_inputs\n","\n","def create_model(model_name, optimizer):\n","    model = TFAutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels=len(label2id),\n","        id2label=id2label,\n","        label2id=label2id,\n","        from_pt=True\n","    )\n","    \n","    model.compile(\n","        optimizer=optimizer,\n","        metrics=[tf.keras.metrics.binary_crossentropy]\n","    )\n","    \n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.868477Z","iopub.status.busy":"2023-08-25T20:55:21.868074Z","iopub.status.idle":"2023-08-25T20:55:22.196953Z","shell.execute_reply":"2023-08-25T20:55:22.195959Z","shell.execute_reply.started":"2023-08-25T20:55:21.868440Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 2500/2500 [00:00<00:00, 22684.57 examples/s]\n"]}],"source":["test = Dataset.from_pandas(test_data)\n","test_tokenized = test.map(tokenize_examples, batched=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:22.199049Z","iopub.status.busy":"2023-08-25T20:55:22.198674Z","iopub.status.idle":"2023-08-25T20:58:30.257943Z","shell.execute_reply":"2023-08-25T20:58:30.256422Z","shell.execute_reply.started":"2023-08-25T20:55:22.199013Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 6000/6000 [00:00<00:00, 25644.43 examples/s]\n","Map: 100%|██████████| 1500/1500 [00:00<00:00, 26240.75 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Downloading pytorch_model.bin: 100%|██████████| 334M/334M [00:16<00:00, 19.7MB/s] \n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_roberta_for_sequence_classification\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFRobertaMainLaye  multiple                  82860288  \n"," r)                                                              \n","                                                                 \n"," classifier (TFRobertaClass  multiple                  592130    \n"," ificationHead)                                                  \n","                                                                 \n","=================================================================\n","Total params: 83452418 (318.35 MB)\n","Trainable params: 83452418 (318.35 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","750/750 [==============================] - 1142s 2s/step - loss: 0.5249 - binary_crossentropy: 4.9616 - val_loss: 0.4374 - val_binary_crossentropy: 5.2345\n","Epoch 2/15\n","750/750 [==============================] - 1230s 2s/step - loss: 0.3443 - binary_crossentropy: 6.0697 - val_loss: 0.4477 - val_binary_crossentropy: 6.5766\n","Epoch 3/15\n","750/750 [==============================] - 1348s 2s/step - loss: 0.2269 - binary_crossentropy: 6.7851 - val_loss: 0.5037 - val_binary_crossentropy: 6.8372\n","Epoch 4/15\n","750/750 [==============================] - 1226s 2s/step - loss: 0.1148 - binary_crossentropy: 7.2633 - val_loss: 0.6700 - val_binary_crossentropy: 6.8519\n","Epoch 4: early stopping\n","188/188 [==============================] - 85s 450ms/step\n","313/313 [==============================] - 149s 475ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 6000/6000 [00:00<00:00, 19289.31 examples/s]\n","Map: 100%|██████████| 1500/1500 [00:00<00:00, 20566.56 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_roberta_for_sequence_classification_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFRobertaMainLaye  multiple                  82860288  \n"," r)                                                              \n","                                                                 \n"," classifier (TFRobertaClass  multiple                  592130    \n"," ificationHead)                                                  \n","                                                                 \n","=================================================================\n","Total params: 83452418 (318.35 MB)\n","Trainable params: 83452418 (318.35 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","750/750 [==============================] - 1183s 2s/step - loss: 0.5218 - binary_crossentropy: 4.5811 - val_loss: 0.4323 - val_binary_crossentropy: 5.1910\n","Epoch 2/15\n","750/750 [==============================] - 1179s 2s/step - loss: 0.3531 - binary_crossentropy: 6.0682 - val_loss: 0.4457 - val_binary_crossentropy: 5.1433\n","Epoch 3/15\n","750/750 [==============================] - 1178s 2s/step - loss: 0.2307 - binary_crossentropy: 6.7129 - val_loss: 0.4907 - val_binary_crossentropy: 6.4773\n","Epoch 4/15\n","750/750 [==============================] - 1161s 2s/step - loss: 0.1183 - binary_crossentropy: 7.2534 - val_loss: 0.6467 - val_binary_crossentropy: 6.5754\n","Epoch 4: early stopping\n","188/188 [==============================] - 84s 444ms/step\n","313/313 [==============================] - 142s 454ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 6000/6000 [00:00<00:00, 19267.63 examples/s]\n","Map: 100%|██████████| 1500/1500 [00:00<00:00, 20527.64 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_roberta_for_sequence_classification_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFRobertaMainLaye  multiple                  82860288  \n"," r)                                                              \n","                                                                 \n"," classifier (TFRobertaClass  multiple                  592130    \n"," ificationHead)                                                  \n","                                                                 \n","=================================================================\n","Total params: 83452418 (318.35 MB)\n","Trainable params: 83452418 (318.35 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","750/750 [==============================] - 1172s 2s/step - loss: 0.5090 - binary_crossentropy: 4.7951 - val_loss: 0.4540 - val_binary_crossentropy: 5.4454\n","Epoch 2/15\n","750/750 [==============================] - 1152s 2s/step - loss: 0.3514 - binary_crossentropy: 6.0300 - val_loss: 0.4583 - val_binary_crossentropy: 6.3056\n","Epoch 3/15\n","750/750 [==============================] - 1155s 2s/step - loss: 0.2396 - binary_crossentropy: 6.6794 - val_loss: 0.5546 - val_binary_crossentropy: 6.3123\n","Epoch 4/15\n","750/750 [==============================] - 1149s 2s/step - loss: 0.1251 - binary_crossentropy: 7.1951 - val_loss: 0.6778 - val_binary_crossentropy: 6.9476\n","Epoch 4: early stopping\n","188/188 [==============================] - 86s 450ms/step\n","313/313 [==============================] - 143s 457ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 6000/6000 [00:00<00:00, 20207.86 examples/s]\n","Map: 100%|██████████| 1500/1500 [00:00<00:00, 19974.72 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_roberta_for_sequence_classification_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFRobertaMainLaye  multiple                  82860288  \n"," r)                                                              \n","                                                                 \n"," classifier (TFRobertaClass  multiple                  592130    \n"," ificationHead)                                                  \n","                                                                 \n","=================================================================\n","Total params: 83452418 (318.35 MB)\n","Trainable params: 83452418 (318.35 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","750/750 [==============================] - 1142s 2s/step - loss: 0.5260 - binary_crossentropy: 4.3773 - val_loss: 0.4083 - val_binary_crossentropy: 5.1092\n","Epoch 2/15\n","750/750 [==============================] - 1138s 2s/step - loss: 0.3541 - binary_crossentropy: 5.7979 - val_loss: 0.3966 - val_binary_crossentropy: 6.0376\n","Epoch 3/15\n","750/750 [==============================] - 1146s 2s/step - loss: 0.2282 - binary_crossentropy: 6.6761 - val_loss: 0.4804 - val_binary_crossentropy: 6.1724\n","Epoch 4/15\n","750/750 [==============================] - 1149s 2s/step - loss: 0.1181 - binary_crossentropy: 7.2247 - val_loss: 0.6431 - val_binary_crossentropy: 6.5464\n","Epoch 5/15\n","750/750 [==============================] - 1150s 2s/step - loss: 0.0591 - binary_crossentropy: 7.4512 - val_loss: 0.7498 - val_binary_crossentropy: 7.1869\n","Epoch 5: early stopping\n","188/188 [==============================] - 89s 467ms/step\n","313/313 [==============================] - 148s 471ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 6000/6000 [00:00<00:00, 20089.01 examples/s]\n","Map: 100%|██████████| 1500/1500 [00:00<00:00, 20922.00 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_roberta_for_sequence_classification_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFRobertaMainLaye  multiple                  82860288  \n"," r)                                                              \n","                                                                 \n"," classifier (TFRobertaClass  multiple                  592130    \n"," ificationHead)                                                  \n","                                                                 \n","=================================================================\n","Total params: 83452418 (318.35 MB)\n","Trainable params: 83452418 (318.35 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","750/750 [==============================] - 1113s 1s/step - loss: 0.5271 - binary_crossentropy: 4.6744 - val_loss: 0.4036 - val_binary_crossentropy: 4.6824\n","Epoch 2/15\n","750/750 [==============================] - 1115s 1s/step - loss: 0.3557 - binary_crossentropy: 5.8670 - val_loss: 0.3934 - val_binary_crossentropy: 6.2201\n","Epoch 3/15\n","750/750 [==============================] - 1131s 2s/step - loss: 0.2267 - binary_crossentropy: 6.7409 - val_loss: 0.4701 - val_binary_crossentropy: 6.9011\n","Epoch 4/15\n","750/750 [==============================] - 1128s 2s/step - loss: 0.1243 - binary_crossentropy: 7.1958 - val_loss: 0.5837 - val_binary_crossentropy: 7.0232\n","Epoch 5/15\n","750/750 [==============================] - 1128s 2s/step - loss: 0.0589 - binary_crossentropy: 7.4447 - val_loss: 0.7081 - val_binary_crossentropy: 7.3089\n","Epoch 5: early stopping\n","188/188 [==============================] - 84s 443ms/step\n","313/313 [==============================] - 147s 468ms/step\n"]}],"source":["fold_path = \"./../inp/sinhala/folds/2023/\"\n","dirs = os.listdir(fold_path)\n","\n","oof_preds = np.zeros((data.shape[0],))\n","test_preds = np.zeros((test_data.shape[0], 2))\n","\n","for dir_name in dirs:\n","    dir_path = os.path.join(fold_path, dir_name)\n","\n","    # Defining the Train and Val paths \n","    train_df = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n","    val_df = pd.read_csv(os.path.join(dir_path, 'val.csv'))\n","    \n","    # Cleaning and Prepareing the Data\n","    train_clean = data_clean(train_df)\n","    val_clean = data_clean(val_df)\n","    \n","    # Converting to HuggingFace Datasets\n","    train_ds = Dataset.from_pandas(train_df)\n","    val_ds = Dataset.from_pandas(val_df)\n","    \n","    # Tokenize the Data    \n","    train_tokenized = train_ds.map(tokenize_examples, batched=True)\n","    val_tokenized = val_ds.map(tokenize_examples, batched=True)\n","    \n","    # Defining the Parameters for Training\n","    batch_size = 8\n","    num_epochs = 10\n","    batches_per_epoch = len(train_tokenized) // batch_size\n","    total_train_steps = int(batches_per_epoch * num_epochs)\n","    optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n","    \n","    # Define the Model \n","    model = create_model(model_name, optimizer)\n","    \n","    # Converting to Tf Dataset for training\n","    train_set = model.prepare_tf_dataset(\n","        train_tokenized,\n","        shuffle=True,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","\n","    validation_set = model.prepare_tf_dataset(\n","        val_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    test_set = model.prepare_tf_dataset(\n","        test_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    # Define Model\n","    es = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        min_delta=0,\n","        patience=3,\n","        verbose=1,\n","        mode='auto',\n","        baseline=None,\n","        restore_best_weights=False,\n","        start_from_epoch=0\n","    )\n","    \n","    history = model.fit(x=train_set,\n","              validation_data=validation_set,\n","              epochs=15,\n","              callbacks=[es])\n","\n","    logits = model.predict(validation_set).logits\n","    y_preds = tf.argmax(tf.nn.sigmoid(logits), axis=1).numpy()\n","    oof_preds[val_df['index'].values] += y_preds\n","\n","    # predict on test\n","    test_logits = model.predict(test_set).logits\n","    test_y_preds = tf.nn.sigmoid(test_logits)\n","    test_preds += test_y_preds/5\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.84      0.81      0.82      4324\n","           1       0.75      0.79      0.77      3176\n","\n","    accuracy                           0.80      7500\n","   macro avg       0.80      0.80      0.80      7500\n","weighted avg       0.80      0.80      0.80      7500\n","\n"]}],"source":["y_true = data['label'].map(label2id)\n","print(classification_report(y_true, oof_preds))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","NOT    1447\n","HOF    1053\n","Name: count, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y_preds_test = tf.argmax(test_preds, axis=1).numpy()\n","test_data['label'] = y_preds_test\n","test_data['label'] = test_data['label'].map(id2label)\n","test_data['label'].value_counts()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>task1a_test_1</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>task1a_test_2</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>task1a_test_3</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>task1a_test_4</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>task1a_test_5</td>\n","      <td>HOF</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              id label\n","0  task1a_test_1   NOT\n","1  task1a_test_2   HOF\n","2  task1a_test_3   HOF\n","3  task1a_test_4   NOT\n","4  task1a_test_5   HOF"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["test_data['id'] = test_id\n","test_data[['id', 'label']].head()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["test_data[['id', 'label']].to_csv('./../out/sinhala_bert_baseline.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
