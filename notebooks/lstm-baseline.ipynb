{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'inp': './../inp', 'out': './../out'}, 'logs': './../logs', 'hyps': {'n_split': 5, 'max_len': {'assamese': 256}, 'random_state': [42, 2023]}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.layers as L\n",
    "from keras import layers\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Layer, Dropout, Dense, Input, Embedding, Bidirectional, LSTM, Concatenate\n",
    "\n",
    "curr_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "proj_dir = os.path.abspath(os.path.join(curr_dir, '..'))\n",
    "\n",
    "sys.path.append(proj_dir)\n",
    "\n",
    "from src.configuration import load_config\n",
    "\n",
    "def seed_everything(seed=2023):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything()\n",
    "\n",
    "cfg = load_config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(cfg['data']['inp'], 'sinhala/train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(cfg['data']['inp'], 'sinhala/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 256\n",
    "VOCAB_SIZE = 20000\n",
    "EMBED_DIM = 128\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5\n",
    "CLASSES = 2\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "TARGET_COL = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train:  7500\n",
      "Length of test:  2500\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train: \", len(train_df))\n",
    "print(\"Length of test: \", len(test_df))\n",
    "\n",
    "tar2num = {'HOF' : 0, 'NOT' : 1}\n",
    "num2tar = {0 : 'HOF', 1 : 'NOT'}\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='!\"#$&(),.:;?@[\\\\]^_`{|}\\t\\n')\n",
    "tokenizer.fit_on_texts(list(train_df[TEXT_COL]) + list(test_df[TEXT_COL]))\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(list(train_df[TEXT_COL]))\n",
    "y_train = train_df[TARGET_COL].map(tar2num)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(list(test_df[TEXT_COL]))\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=MAXLEN)\n",
    "X_test = pad_sequences(X_test, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(keras.Model):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.W1 = Dense(units=units)\n",
    "        self.W2 = Dense(units=units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_t = K.expand_dims(hidden, 1)\n",
    "        # additive attention\n",
    "        score = K.tanh(self.W1(features) + self.W2(hidden_t))\n",
    "\n",
    "        attn_weights = K.softmax(self.V(score), axis=1)\n",
    "\n",
    "        context = attn_weights * features\n",
    "        context = tf.reduce_sum(context, axis=1)\n",
    "\n",
    "        return context, attn_weights\n",
    "        pass\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_model(max_len, max_features, embed_size, attn_units=20, num_classes=4, rnn_cell_size=32):\n",
    "    seq_inp = Input(shape=max_len, dtype=\"int32\")\n",
    "    embedded_seq = Embedding(max_features, embed_size)(seq_inp)\n",
    "    lstm = Bidirectional(LSTM(\n",
    "        rnn_cell_size, return_sequences=True\n",
    "    ), name=\"bilstm_0\")(embedded_seq)\n",
    "\n",
    "    lstm, f_h, f_c, b_h, b_c = Bidirectional(LSTM(\n",
    "        rnn_cell_size, return_sequences=True, return_state=True\n",
    "    ), name=\"bilstm_1\")(lstm)\n",
    "\n",
    "    h_ = Concatenate()([f_h, b_h])\n",
    "    c_ = Concatenate()([f_c, b_c])\n",
    "\n",
    "    context, attn_weights = AttentionBlock(attn_units)(lstm, h_)\n",
    "\n",
    "    fc_pre = Dense(num_classes * 4, activation=\"relu\")(context)\n",
    "    do = Dropout(0.05)(fc_pre)\n",
    "    output = Dense(1, activation=\"sigmoid\")(do)\n",
    "\n",
    "    return keras.Model(inputs=seq_inp, outputs=output)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 256, 128)             2560000   ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " bilstm_0 (Bidirectional)    (None, 256, 64)              41216     ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " bilstm_1 (Bidirectional)    [(None, 256, 64),            24832     ['bilstm_0[0][0]']            \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32)]                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 64)                   0         ['bilstm_1[0][1]',            \n",
      "                                                                     'bilstm_1[0][3]']            \n",
      "                                                                                                  \n",
      " attention_block (Attention  ((None, 64),                 2621      ['bilstm_1[0][0]',            \n",
      " Block)                       (None, 256, 1))                        'concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 8)                    520       ['attention_block[0][0]']     \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 8)                    0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 1)                    9         ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2629198 (10.03 MB)\n",
      "Trainable params: 2629198 (10.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_len=MAXLEN, max_features=VOCAB_SIZE, embed_size=EMBED_DIM, num_classes=CLASSES)\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[tf.keras.metrics.binary_crossentropy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======FOLD 0=====\n",
      "\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 14s 259ms/step - loss: 0.6916 - binary_crossentropy: 0.6916 - val_loss: 0.6863 - val_binary_crossentropy: 0.6863\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 12s 249ms/step - loss: 0.6817 - binary_crossentropy: 0.6817 - val_loss: 0.6670 - val_binary_crossentropy: 0.6670\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 11s 245ms/step - loss: 0.4914 - binary_crossentropy: 0.4914 - val_loss: 0.4707 - val_binary_crossentropy: 0.4707\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 11s 243ms/step - loss: 0.2263 - binary_crossentropy: 0.2263 - val_loss: 0.5802 - val_binary_crossentropy: 0.5802\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 11s 239ms/step - loss: 0.1043 - binary_crossentropy: 0.1043 - val_loss: 0.8041 - val_binary_crossentropy: 0.8041\n",
      "47/47 [==============================] - 1s 22ms/step\n",
      "79/79 [==============================] - 2s 21ms/step\n",
      "\n",
      "======FOLD 1=====\n",
      "\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 12s 243ms/step - loss: 0.2163 - binary_crossentropy: 0.2163 - val_loss: 0.0783 - val_binary_crossentropy: 0.0783\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 11s 236ms/step - loss: 0.0744 - binary_crossentropy: 0.0744 - val_loss: 0.0829 - val_binary_crossentropy: 0.0829\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 11s 234ms/step - loss: 0.0381 - binary_crossentropy: 0.0381 - val_loss: 0.0977 - val_binary_crossentropy: 0.0977\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 11s 236ms/step - loss: 0.0232 - binary_crossentropy: 0.0232 - val_loss: 0.1090 - val_binary_crossentropy: 0.1090\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 11s 234ms/step - loss: 0.0130 - binary_crossentropy: 0.0130 - val_loss: 0.1154 - val_binary_crossentropy: 0.1154\n",
      "47/47 [==============================] - 1s 21ms/step\n",
      "79/79 [==============================] - 2s 20ms/step\n",
      "\n",
      "======FOLD 2=====\n",
      "\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 12s 236ms/step - loss: 0.0408 - binary_crossentropy: 0.0408 - val_loss: 0.0152 - val_binary_crossentropy: 0.0152\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 11s 233ms/step - loss: 0.0183 - binary_crossentropy: 0.0183 - val_loss: 0.0290 - val_binary_crossentropy: 0.0290\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 11s 224ms/step - loss: 0.0170 - binary_crossentropy: 0.0170 - val_loss: 0.0273 - val_binary_crossentropy: 0.0273\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 11s 228ms/step - loss: 0.0134 - binary_crossentropy: 0.0134 - val_loss: 0.0293 - val_binary_crossentropy: 0.0293\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 11s 228ms/step - loss: 0.0105 - binary_crossentropy: 0.0105 - val_loss: 0.0253 - val_binary_crossentropy: 0.0253\n",
      "47/47 [==============================] - 1s 22ms/step\n",
      "79/79 [==============================] - 2s 21ms/step\n",
      "\n",
      "======FOLD 3=====\n",
      "\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 12s 234ms/step - loss: 0.0131 - binary_crossentropy: 0.0131 - val_loss: 0.0156 - val_binary_crossentropy: 0.0156\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 11s 232ms/step - loss: 0.0123 - binary_crossentropy: 0.0123 - val_loss: 0.0238 - val_binary_crossentropy: 0.0238\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 12s 260ms/step - loss: 0.0133 - binary_crossentropy: 0.0133 - val_loss: 0.0277 - val_binary_crossentropy: 0.0277\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 12s 263ms/step - loss: 0.0106 - binary_crossentropy: 0.0106 - val_loss: 0.0410 - val_binary_crossentropy: 0.0410\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 14s 309ms/step - loss: 0.0074 - binary_crossentropy: 0.0074 - val_loss: 0.0368 - val_binary_crossentropy: 0.0368\n",
      "47/47 [==============================] - 1s 25ms/step\n",
      "79/79 [==============================] - 2s 29ms/step\n",
      "\n",
      "======FOLD 4=====\n",
      "\n",
      "Epoch 1/5\n",
      "47/47 [==============================] - 16s 317ms/step - loss: 0.0139 - binary_crossentropy: 0.0139 - val_loss: 0.0025 - val_binary_crossentropy: 0.0025\n",
      "Epoch 2/5\n",
      "47/47 [==============================] - 16s 350ms/step - loss: 0.0105 - binary_crossentropy: 0.0105 - val_loss: 0.0029 - val_binary_crossentropy: 0.0029\n",
      "Epoch 3/5\n",
      "47/47 [==============================] - 16s 335ms/step - loss: 0.0073 - binary_crossentropy: 0.0073 - val_loss: 0.0035 - val_binary_crossentropy: 0.0035\n",
      "Epoch 4/5\n",
      "47/47 [==============================] - 15s 325ms/step - loss: 0.0071 - binary_crossentropy: 0.0071 - val_loss: 0.0051 - val_binary_crossentropy: 0.0051\n",
      "Epoch 5/5\n",
      "47/47 [==============================] - 16s 340ms/step - loss: 0.0064 - binary_crossentropy: 0.0064 - val_loss: 0.0035 - val_binary_crossentropy: 0.0035\n",
      "47/47 [==============================] - 2s 28ms/step\n",
      "79/79 [==============================] - 2s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "cv_splits = list(StratifiedKFold(n_splits=5).split(X_train, y_train))\n",
    "\n",
    "oof_preds = np.zeros((X_train.shape[0],))\n",
    "test_preds = np.zeros((X_test.shape[0],))\n",
    "\n",
    "for fold in range(5):\n",
    "    K.clear_session()\n",
    "    train_idx, val_idx = cv_splits[fold]\n",
    "    print(\"\\n======FOLD {}=====\".format(fold))\n",
    "    print()\n",
    "    model.fit(X_train[train_idx], y_train[train_idx],\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=EPOCHS,\n",
    "                validation_data=(X_train[val_idx], y_train[val_idx]))\n",
    "\n",
    "    oof_preds[val_idx] += model.predict(X_train[val_idx])[:, 0]\n",
    "    test_preds += model.predict(X_test)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished...\n",
      "Performance in training Data...\n",
      "F1 Score for Training:  0.940048392937221\n",
      "Classification report for training: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      3176\n",
      "           1       0.95      0.94      0.95      4324\n",
      "\n",
      "    accuracy                           0.94      7500\n",
      "   macro avg       0.94      0.94      0.94      7500\n",
      "weighted avg       0.94      0.94      0.94      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Finished...\")\n",
    "print(\"Performance in training Data...\")\n",
    "oof_pred = tf.math.greater_equal(oof_preds, 0.5)\n",
    "val_f1_score = f1_score(y_true=y_train, y_pred=oof_pred, average='macro')\n",
    "print(\"F1 Score for Training: \", val_f1_score)\n",
    "print(\"Classification report for training: \\n\", classification_report(y_true=y_train, y_pred=oof_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test data...\n"
     ]
    }
   ],
   "source": [
    "y_preds = test_preds / 5\n",
    "\n",
    "print(\"Evaluation on test data...\")\n",
    "y_pred = tf.math.greater_equal(y_preds, 0.5).numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NOT    1481\n",
       "HOF    1019\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[TARGET_COL] = y_pred\n",
    "test_df[TARGET_COL] = test_df[TARGET_COL].map(num2tar)\n",
    "test_df[TARGET_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[['post_id', 'label']].rename(columns={'post_id': 'id'}).to_csv('./../out/guj_test_baseline.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hasoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
