{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:54:53.110224Z","iopub.status.busy":"2023-08-25T20:54:53.109798Z","iopub.status.idle":"2023-08-25T20:55:19.953289Z","shell.execute_reply":"2023-08-25T20:55:19.952131Z","shell.execute_reply.started":"2023-08-25T20:54:53.110178Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/mrutyunjaybiswal/Documents/rcml/hasoc23-hate-speech-bengali-bodo-assamese/hasoc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import re\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorWithPadding\n","from transformers import create_optimizer\n","from transformers import TFAutoModelForSequenceClassification\n","\n","from sklearn.metrics import classification_report\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:19.956512Z","iopub.status.busy":"2023-08-25T20:55:19.956139Z","iopub.status.idle":"2023-08-25T20:55:20.011716Z","shell.execute_reply":"2023-08-25T20:55:20.010753Z","shell.execute_reply.started":"2023-08-25T20:55:19.956477Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"./../inp/gujju/train.csv\")\n","test_data = pd.read_csv(\"./../inp/gujju/test.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.085228Z","iopub.status.busy":"2023-08-25T20:55:20.084904Z","iopub.status.idle":"2023-08-25T20:55:20.089737Z","shell.execute_reply":"2023-08-25T20:55:20.088734Z","shell.execute_reply.started":"2023-08-25T20:55:20.085196Z"},"trusted":true},"outputs":[],"source":["id2label = {0: \"NOT\", 1: \"HOF\"}\n","label2id = {\"NOT\": 0, \"HOF\": 1}"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.092143Z","iopub.status.busy":"2023-08-25T20:55:20.091394Z","iopub.status.idle":"2023-08-25T20:55:20.100560Z","shell.execute_reply":"2023-08-25T20:55:20.099676Z","shell.execute_reply.started":"2023-08-25T20:55:20.092084Z"},"trusted":true},"outputs":[],"source":["def data_clean(data_df):\n","    # Removing Unwanted Columns\n","    data_df.drop([\"tweet_id\", \"created_at\", \"user_screen_name\"], axis=1, inplace=True)\n","    \n","    # Removing @tags \n","    pattern = r'@\\w+'\n","    data_df[\"text\"] = data_df[\"text\"].apply(lambda x: re.sub(pattern, '', x))\n","    \n","    # Transforming Categorical Values to Numericals\n","    data_df[\"labels\"] = data_df[\"label\"].apply(lambda x: [label2id[x]])\n","    \n","    # Dropping label column\n","    data_df.drop(\"label\", axis=1, inplace=True)\n","    \n","    return data_df"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.104445Z","iopub.status.busy":"2023-08-25T20:55:20.104144Z","iopub.status.idle":"2023-08-25T20:55:20.123435Z","shell.execute_reply":"2023-08-25T20:55:20.122458Z","shell.execute_reply.started":"2023-08-25T20:55:20.104416Z"},"trusted":true},"outputs":[],"source":["# Cleaning and Preparing Test Data\n","test_id = test_data[\"tweet_id\"]\n","test_data.drop(\"tweet_id\", axis=1, inplace=True)\n","pattern = r'@\\w+'\n","test_data[\"text\"] = test_data[\"text\"].apply(lambda x: re.sub(pattern, '', x))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.125659Z","iopub.status.busy":"2023-08-25T20:55:20.125221Z","iopub.status.idle":"2023-08-25T20:55:21.347551Z","shell.execute_reply":"2023-08-25T20:55:21.346508Z","shell.execute_reply.started":"2023-08-25T20:55:20.125573Z"},"trusted":true},"outputs":[],"source":["model_name = \"l3cube-pune/gujarati-bert\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, from_pt=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.349436Z","iopub.status.busy":"2023-08-25T20:55:21.348943Z","iopub.status.idle":"2023-08-25T20:55:21.866335Z","shell.execute_reply":"2023-08-25T20:55:21.865190Z","shell.execute_reply.started":"2023-08-25T20:55:21.349391Z"},"trusted":true},"outputs":[],"source":["def tokenize_examples(examples):\n","    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, max_length=True)\n","    return tokenized_inputs\n","\n","def create_model(model_name, optimizer):\n","    model = TFAutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels=len(label2id),\n","        id2label=id2label,\n","        label2id=label2id,\n","        from_pt=True\n","    )\n","    \n","    model.compile(\n","        optimizer=optimizer,\n","        metrics=[tf.keras.metrics.binary_crossentropy]\n","    )\n","    \n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.868477Z","iopub.status.busy":"2023-08-25T20:55:21.868074Z","iopub.status.idle":"2023-08-25T20:55:22.196953Z","shell.execute_reply":"2023-08-25T20:55:22.195959Z","shell.execute_reply.started":"2023-08-25T20:55:21.868440Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 1196/1196 [00:00<00:00, 9605.96 examples/s]\n"]}],"source":["test = Dataset.from_pandas(test_data)\n","test_tokenized = test.map(tokenize_examples, batched=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:22.199049Z","iopub.status.busy":"2023-08-25T20:55:22.198674Z","iopub.status.idle":"2023-08-25T20:58:30.257943Z","shell.execute_reply":"2023-08-25T20:58:30.256422Z","shell.execute_reply.started":"2023-08-25T20:55:22.199013Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 24648.82 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 12678.32 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_37 (Dropout)        multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","20/20 [==============================] - 55s 2s/step - loss: 0.6930 - binary_crossentropy: 6.6702 - val_loss: 0.6925 - val_binary_crossentropy: 7.0235\n","Epoch 2/15\n","20/20 [==============================] - 47s 2s/step - loss: 0.6904 - binary_crossentropy: 6.6150 - val_loss: 0.6883 - val_binary_crossentropy: 5.3221\n","Epoch 3/15\n","20/20 [==============================] - 47s 2s/step - loss: 0.6797 - binary_crossentropy: 4.9572 - val_loss: 0.6798 - val_binary_crossentropy: 5.2000\n","Epoch 4/15\n","20/20 [==============================] - 47s 2s/step - loss: 0.6591 - binary_crossentropy: 4.7729 - val_loss: 0.6718 - val_binary_crossentropy: 4.7517\n","Epoch 5/15\n","20/20 [==============================] - 48s 2s/step - loss: 0.6273 - binary_crossentropy: 4.5512 - val_loss: 0.6534 - val_binary_crossentropy: 4.6211\n","Epoch 6/15\n","20/20 [==============================] - 48s 2s/step - loss: 0.5878 - binary_crossentropy: 4.4034 - val_loss: 0.6511 - val_binary_crossentropy: 4.3648\n","Epoch 7/15\n","20/20 [==============================] - 49s 2s/step - loss: 0.5464 - binary_crossentropy: 4.3335 - val_loss: 0.6494 - val_binary_crossentropy: 4.3741\n","Epoch 8/15\n","20/20 [==============================] - 48s 2s/step - loss: 0.5220 - binary_crossentropy: 4.3329 - val_loss: 0.6149 - val_binary_crossentropy: 4.2896\n","Epoch 9/15\n","20/20 [==============================] - 49s 2s/step - loss: 0.4972 - binary_crossentropy: 4.2993 - val_loss: 0.6278 - val_binary_crossentropy: 4.3144\n","Epoch 10/15\n","20/20 [==============================] - 51s 3s/step - loss: 0.4864 - binary_crossentropy: 4.2873 - val_loss: 0.6226 - val_binary_crossentropy: 4.3311\n","Epoch 11/15\n","20/20 [==============================] - 49s 2s/step - loss: 0.4845 - binary_crossentropy: 4.2872 - val_loss: 0.6226 - val_binary_crossentropy: 4.3311\n","Epoch 11: early stopping\n","5/5 [==============================] - 3s 377ms/step\n","150/150 [==============================] - 61s 398ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 6851.41 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 10663.71 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_75 (Dropout)        multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 57s 3s/step - loss: 0.6928 - binary_crossentropy: 6.7334 - val_loss: 0.6926 - val_binary_crossentropy: 7.7374\n","Epoch 2/15\n","20/20 [==============================] - 50s 3s/step - loss: 0.6898 - binary_crossentropy: 6.4372 - val_loss: 0.6892 - val_binary_crossentropy: 7.2259\n","Epoch 3/15\n","20/20 [==============================] - 53s 3s/step - loss: 0.6798 - binary_crossentropy: 5.5383 - val_loss: 0.6775 - val_binary_crossentropy: 5.6139\n","Epoch 4/15\n","20/20 [==============================] - 53s 3s/step - loss: 0.6544 - binary_crossentropy: 4.7040 - val_loss: 0.6609 - val_binary_crossentropy: 5.1432\n","Epoch 5/15\n","20/20 [==============================] - 52s 3s/step - loss: 0.6201 - binary_crossentropy: 4.4236 - val_loss: 0.6488 - val_binary_crossentropy: 4.8705\n","Epoch 6/15\n","20/20 [==============================] - 45s 2s/step - loss: 0.5736 - binary_crossentropy: 4.3382 - val_loss: 0.6350 - val_binary_crossentropy: 4.7811\n","Epoch 7/15\n","20/20 [==============================] - 45s 2s/step - loss: 0.5363 - binary_crossentropy: 4.2774 - val_loss: 0.6299 - val_binary_crossentropy: 4.7460\n","Epoch 8/15\n","20/20 [==============================] - 48s 2s/step - loss: 0.5122 - binary_crossentropy: 4.2230 - val_loss: 0.6234 - val_binary_crossentropy: 4.7368\n","Epoch 9/15\n","20/20 [==============================] - 48s 2s/step - loss: 0.4914 - binary_crossentropy: 4.2065 - val_loss: 0.6228 - val_binary_crossentropy: 4.7095\n","Epoch 10/15\n","20/20 [==============================] - 49s 2s/step - loss: 0.4847 - binary_crossentropy: 4.2029 - val_loss: 0.6182 - val_binary_crossentropy: 4.6078\n","Epoch 11/15\n","20/20 [==============================] - 52s 3s/step - loss: 0.4823 - binary_crossentropy: 4.2045 - val_loss: 0.6182 - val_binary_crossentropy: 4.6078\n","Epoch 12/15\n","20/20 [==============================] - 51s 3s/step - loss: 0.4788 - binary_crossentropy: 4.1954 - val_loss: 0.6182 - val_binary_crossentropy: 4.6078\n","Epoch 13/15\n","20/20 [==============================] - 53s 3s/step - loss: 0.4798 - binary_crossentropy: 4.2061 - val_loss: 0.6182 - val_binary_crossentropy: 4.6078\n","Epoch 13: early stopping\n","5/5 [==============================] - 3s 333ms/step\n","150/150 [==============================] - 66s 437ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 6088.96 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 6829.17 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_113 (Dropout)       multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 57s 3s/step - loss: 0.6931 - binary_crossentropy: 6.2504 - val_loss: 0.6925 - val_binary_crossentropy: 6.9296\n","Epoch 2/15\n","20/20 [==============================] - 54s 3s/step - loss: 0.6914 - binary_crossentropy: 5.5436 - val_loss: 0.6897 - val_binary_crossentropy: 5.5189\n","Epoch 3/15\n","20/20 [==============================] - 55s 3s/step - loss: 0.6821 - binary_crossentropy: 5.5199 - val_loss: 0.6921 - val_binary_crossentropy: 5.1800\n","Epoch 4/15\n","20/20 [==============================] - 52s 3s/step - loss: 0.6653 - binary_crossentropy: 5.0769 - val_loss: 0.6843 - val_binary_crossentropy: 5.3322\n","Epoch 5/15\n","20/20 [==============================] - 53s 3s/step - loss: 0.6472 - binary_crossentropy: 5.1856 - val_loss: 0.6794 - val_binary_crossentropy: 5.1145\n","Epoch 6/15\n","20/20 [==============================] - 53s 3s/step - loss: 0.6238 - binary_crossentropy: 5.0168 - val_loss: 0.6800 - val_binary_crossentropy: 5.0063\n","Epoch 7/15\n","20/20 [==============================] - 53s 3s/step - loss: 0.6100 - binary_crossentropy: 4.8109 - val_loss: 0.6706 - val_binary_crossentropy: 5.0312\n","Epoch 8/15\n","20/20 [==============================] - 52s 3s/step - loss: 0.5962 - binary_crossentropy: 4.7697 - val_loss: 0.6723 - val_binary_crossentropy: 4.8839\n","Epoch 9/15\n","20/20 [==============================] - 51s 3s/step - loss: 0.5884 - binary_crossentropy: 4.7342 - val_loss: 0.6796 - val_binary_crossentropy: 4.9766\n","Epoch 10/15\n","20/20 [==============================] - 52s 3s/step - loss: 0.5829 - binary_crossentropy: 4.7301 - val_loss: 0.6770 - val_binary_crossentropy: 4.8498\n","Epoch 10: early stopping\n","5/5 [==============================] - 3s 456ms/step\n","150/150 [==============================] - 70s 456ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 14780.39 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 8512.03 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_151 (Dropout)       multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.6933 - binary_crossentropy: 5.0701 - val_loss: 0.6916 - val_binary_crossentropy: 3.1313\n","Epoch 2/15\n","20/20 [==============================] - 53s 3s/step - loss: 0.6886 - binary_crossentropy: 4.9786 - val_loss: 0.7004 - val_binary_crossentropy: 3.2094\n","Epoch 3/15\n","20/20 [==============================] - 55s 3s/step - loss: 0.6761 - binary_crossentropy: 4.8656 - val_loss: 0.7112 - val_binary_crossentropy: 2.9788\n","Epoch 4/15\n","20/20 [==============================] - 57s 3s/step - loss: 0.6606 - binary_crossentropy: 4.8130 - val_loss: 0.6966 - val_binary_crossentropy: 3.4112\n","Epoch 4: early stopping\n","5/5 [==============================] - 4s 566ms/step\n","150/150 [==============================] - 81s 535ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 5588.17 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 7259.72 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_189 (Dropout)       multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 69s 3s/step - loss: 0.6933 - binary_crossentropy: 4.6850 - val_loss: 0.6930 - val_binary_crossentropy: 6.4008\n","Epoch 2/15\n","20/20 [==============================] - 70s 4s/step - loss: 0.6927 - binary_crossentropy: 4.8730 - val_loss: 0.6921 - val_binary_crossentropy: 6.0400\n","Epoch 3/15\n","20/20 [==============================] - 70s 4s/step - loss: 0.6886 - binary_crossentropy: 4.9140 - val_loss: 0.6876 - val_binary_crossentropy: 5.6078\n","Epoch 4/15\n","20/20 [==============================] - 75s 4s/step - loss: 0.6786 - binary_crossentropy: 4.8143 - val_loss: 0.6822 - val_binary_crossentropy: 5.6381\n","Epoch 5/15\n","20/20 [==============================] - 75s 4s/step - loss: 0.6595 - binary_crossentropy: 4.9011 - val_loss: 0.6850 - val_binary_crossentropy: 5.4494\n","Epoch 6/15\n","20/20 [==============================] - 75s 4s/step - loss: 0.6373 - binary_crossentropy: 4.6373 - val_loss: 0.6783 - val_binary_crossentropy: 5.3925\n","Epoch 7/15\n","20/20 [==============================] - 77s 4s/step - loss: 0.6085 - binary_crossentropy: 4.4982 - val_loss: 0.6771 - val_binary_crossentropy: 5.2588\n","Epoch 8/15\n","20/20 [==============================] - 77s 4s/step - loss: 0.5773 - binary_crossentropy: 4.3582 - val_loss: 0.6755 - val_binary_crossentropy: 5.0916\n","Epoch 9/15\n","20/20 [==============================] - 77s 4s/step - loss: 0.5564 - binary_crossentropy: 4.2732 - val_loss: 0.6791 - val_binary_crossentropy: 5.2204\n","Epoch 10/15\n","20/20 [==============================] - 79s 4s/step - loss: 0.5416 - binary_crossentropy: 4.2554 - val_loss: 0.6806 - val_binary_crossentropy: 5.0244\n","Epoch 11/15\n","20/20 [==============================] - 77s 4s/step - loss: 0.5421 - binary_crossentropy: 4.2583 - val_loss: 0.6806 - val_binary_crossentropy: 5.0244\n","Epoch 11: early stopping\n","5/5 [==============================] - 5s 646ms/step\n","150/150 [==============================] - 97s 641ms/step\n"]}],"source":["fold_path = \"./../inp/gujju/folds/2023/\"\n","dirs = os.listdir(fold_path)\n","\n","oof_preds = np.zeros((data.shape[0],))\n","test_preds = np.zeros((test_data.shape[0], 2))\n","\n","for dir_name in dirs:\n","    dir_path = os.path.join(fold_path, dir_name)\n","\n","    # Defining the Train and Val paths \n","    train_df = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n","    val_df = pd.read_csv(os.path.join(dir_path, 'val.csv'))\n","    \n","    # Cleaning and Prepareing the Data\n","    train_clean = data_clean(train_df)\n","    val_clean = data_clean(val_df)\n","    \n","    # Converting to HuggingFace Datasets\n","    train_ds = Dataset.from_pandas(train_df)\n","    val_ds = Dataset.from_pandas(val_df)\n","    \n","    # Tokenize the Data    \n","    train_tokenized = train_ds.map(tokenize_examples, batched=True)\n","    val_tokenized = val_ds.map(tokenize_examples, batched=True)\n","    \n","    # Defining the Parameters for Training\n","    batch_size = 8\n","    num_epochs = 10\n","    batches_per_epoch = len(train_tokenized) // batch_size\n","    total_train_steps = int(batches_per_epoch * num_epochs)\n","    optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n","    \n","    # Define the Model \n","    model = create_model(model_name, optimizer)\n","    \n","    # Converting to Tf Dataset for training\n","    train_set = model.prepare_tf_dataset(\n","        train_tokenized,\n","        shuffle=True,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","\n","    validation_set = model.prepare_tf_dataset(\n","        val_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    test_set = model.prepare_tf_dataset(\n","        test_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    # Define Model\n","    es = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        min_delta=0,\n","        patience=3,\n","        verbose=1,\n","        mode='auto',\n","        baseline=None,\n","        restore_best_weights=False,\n","        start_from_epoch=0\n","    )\n","    \n","    history = model.fit(x=train_set,\n","              validation_data=validation_set,\n","              epochs=15,\n","              callbacks=[es])\n","\n","    logits = model.predict(validation_set).logits\n","    y_preds = tf.argmax(tf.nn.sigmoid(logits), axis=1).numpy()\n","    oof_preds[val_df['index'].values] += y_preds\n","\n","    # predict on test\n","    test_logits = model.predict(test_set).logits\n","    test_y_preds = tf.nn.sigmoid(test_logits)\n","    test_preds += test_y_preds/5\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.60      0.67      0.63       100\n","           1       0.62      0.55      0.59       100\n","\n","    accuracy                           0.61       200\n","   macro avg       0.61      0.61      0.61       200\n","weighted avg       0.61      0.61      0.61       200\n","\n"]}],"source":["y_true = data['label'].map(label2id)\n","print(classification_report(y_true, oof_preds))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","NOT    718\n","HOF    478\n","Name: count, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y_preds_test = tf.argmax(test_preds, axis=1).numpy()\n","test_data['label'] = y_preds_test\n","test_data['label'] = test_data['label'].map(id2label)\n","test_data['label'].value_counts()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023_test_main_tweet_1</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023_test_main_tweet_2</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023_test_main_tweet_3</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023_test_main_tweet_4</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023_test_main_tweet_5</td>\n","      <td>HOF</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       id label\n","0  2023_test_main_tweet_1   NOT\n","1  2023_test_main_tweet_2   HOF\n","2  2023_test_main_tweet_3   HOF\n","3  2023_test_main_tweet_4   NOT\n","4  2023_test_main_tweet_5   HOF"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["test_data['id'] = test_id\n","test_data[['id', 'label']].head()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["test_data[['id', 'label']].to_csv('./../out/guj_bert_baseline.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
