{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:54:53.110224Z","iopub.status.busy":"2023-08-25T20:54:53.109798Z","iopub.status.idle":"2023-08-25T20:55:19.953289Z","shell.execute_reply":"2023-08-25T20:55:19.952131Z","shell.execute_reply.started":"2023-08-25T20:54:53.110178Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/mrutyunjaybiswal/Documents/rcml/hasoc23-hate-speech-bengali-bodo-assamese/hasoc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import re\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorWithPadding\n","from transformers import create_optimizer\n","from transformers import TFAutoModelForSequenceClassification\n","\n","from sklearn.metrics import classification_report\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:19.956512Z","iopub.status.busy":"2023-08-25T20:55:19.956139Z","iopub.status.idle":"2023-08-25T20:55:20.011716Z","shell.execute_reply":"2023-08-25T20:55:20.010753Z","shell.execute_reply.started":"2023-08-25T20:55:19.956477Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"./../inp/gujju/train.csv\")\n","test_data = pd.read_csv(\"./../inp/gujju/test.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.085228Z","iopub.status.busy":"2023-08-25T20:55:20.084904Z","iopub.status.idle":"2023-08-25T20:55:20.089737Z","shell.execute_reply":"2023-08-25T20:55:20.088734Z","shell.execute_reply.started":"2023-08-25T20:55:20.085196Z"},"trusted":true},"outputs":[],"source":["id2label = {0: \"NOT\", 1: \"HOF\"}\n","label2id = {\"NOT\": 0, \"HOF\": 1}"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.092143Z","iopub.status.busy":"2023-08-25T20:55:20.091394Z","iopub.status.idle":"2023-08-25T20:55:20.100560Z","shell.execute_reply":"2023-08-25T20:55:20.099676Z","shell.execute_reply.started":"2023-08-25T20:55:20.092084Z"},"trusted":true},"outputs":[],"source":["def data_clean(data_df):\n","    # Removing Unwanted Columns\n","    data_df.drop([\"tweet_id\", \"created_at\", \"user_screen_name\"], axis=1, inplace=True)\n","    \n","    # Removing @tags \n","    pattern = r'@\\w+'\n","    data_df[\"text\"] = data_df[\"text\"].apply(lambda x: re.sub(pattern, '', x))\n","    \n","    # Transforming Categorical Values to Numericals\n","    data_df[\"labels\"] = data_df[\"label\"].apply(lambda x: [label2id[x]])\n","    \n","    # Dropping label column\n","    data_df.drop(\"label\", axis=1, inplace=True)\n","    \n","    return data_df"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.104445Z","iopub.status.busy":"2023-08-25T20:55:20.104144Z","iopub.status.idle":"2023-08-25T20:55:20.123435Z","shell.execute_reply":"2023-08-25T20:55:20.122458Z","shell.execute_reply.started":"2023-08-25T20:55:20.104416Z"},"trusted":true},"outputs":[],"source":["# Cleaning and Preparing Test Data\n","test_id = test_data[\"tweet_id\"]\n","test_data.drop(\"tweet_id\", axis=1, inplace=True)\n","pattern = r'@\\w+'\n","test_data[\"text\"] = test_data[\"text\"].apply(lambda x: re.sub(pattern, '', x))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.125659Z","iopub.status.busy":"2023-08-25T20:55:20.125221Z","iopub.status.idle":"2023-08-25T20:55:21.347551Z","shell.execute_reply":"2023-08-25T20:55:21.346508Z","shell.execute_reply.started":"2023-08-25T20:55:20.125573Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading (…)lve/main/config.json: 100%|██████████| 486/486 [00:00<00:00, 1.61MB/s]\n","Downloading (…)tencepiece.bpe.model: 100%|██████████| 5.07M/5.07M [00:01<00:00, 3.59MB/s]\n","Downloading (…)/main/tokenizer.json: 100%|██████████| 9.10M/9.10M [00:01<00:00, 5.63MB/s]\n"]}],"source":["model_name = \"ashwani-tanwar/Gujarati-XLM-R-Base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, from_pt=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.349436Z","iopub.status.busy":"2023-08-25T20:55:21.348943Z","iopub.status.idle":"2023-08-25T20:55:21.866335Z","shell.execute_reply":"2023-08-25T20:55:21.865190Z","shell.execute_reply.started":"2023-08-25T20:55:21.349391Z"},"trusted":true},"outputs":[],"source":["def tokenize_examples(examples):\n","    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, max_length=True)\n","    return tokenized_inputs\n","\n","def create_model(model_name, optimizer):\n","    model = TFAutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels=len(label2id),\n","        id2label=id2label,\n","        label2id=label2id,\n","        from_pt=True\n","    )\n","    \n","    model.compile(\n","        optimizer=optimizer,\n","        metrics=[tf.keras.metrics.binary_crossentropy]\n","    )\n","    \n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.868477Z","iopub.status.busy":"2023-08-25T20:55:21.868074Z","iopub.status.idle":"2023-08-25T20:55:22.196953Z","shell.execute_reply":"2023-08-25T20:55:22.195959Z","shell.execute_reply.started":"2023-08-25T20:55:21.868440Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 1196/1196 [00:00<00:00, 17211.89 examples/s]\n"]}],"source":["test = Dataset.from_pandas(test_data)\n","test_tokenized = test.map(tokenize_examples, batched=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:22.199049Z","iopub.status.busy":"2023-08-25T20:55:22.198674Z","iopub.status.idle":"2023-08-25T20:58:30.257943Z","shell.execute_reply":"2023-08-25T20:58:30.256422Z","shell.execute_reply.started":"2023-08-25T20:55:22.199013Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 1298.98 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 13096.97 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Downloading pytorch_model.bin: 100%|██████████| 1.12G/1.12G [00:38<00:00, 29.1MB/s]\n","All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tfxlm_roberta_for_sequence_classification\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFXLMRobertaMainL  multiple                  277453056 \n"," ayer)                                                           \n","                                                                 \n"," classifier (TFXLMRobertaCl  multiple                  592130    \n"," assificationHead)                                               \n","                                                                 \n","=================================================================\n","Total params: 278045186 (1.04 GB)\n","Trainable params: 278045186 (1.04 GB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","20/20 [==============================] - 113s 5s/step - loss: 0.7012 - binary_crossentropy: 5.8788 - val_loss: 0.6942 - val_binary_crossentropy: 7.7125\n","Epoch 2/15\n","20/20 [==============================] - 168s 8s/step - loss: 0.6893 - binary_crossentropy: 6.5463 - val_loss: 0.6904 - val_binary_crossentropy: 7.7125\n","Epoch 3/15\n","20/20 [==============================] - 172s 9s/step - loss: 0.6525 - binary_crossentropy: 6.7602 - val_loss: 0.6663 - val_binary_crossentropy: 4.7395\n","Epoch 4/15\n","20/20 [==============================] - 169s 8s/step - loss: 0.5476 - binary_crossentropy: 5.1590 - val_loss: 0.7147 - val_binary_crossentropy: 5.1170\n","Epoch 5/15\n","20/20 [==============================] - 107s 5s/step - loss: 0.4976 - binary_crossentropy: 4.8747 - val_loss: 0.6539 - val_binary_crossentropy: 4.0426\n","Epoch 6/15\n","20/20 [==============================] - 102s 5s/step - loss: 0.4698 - binary_crossentropy: 4.8426 - val_loss: 0.6844 - val_binary_crossentropy: 5.3701\n","Epoch 7/15\n","20/20 [==============================] - 104s 5s/step - loss: 0.3918 - binary_crossentropy: 6.4994 - val_loss: 0.8234 - val_binary_crossentropy: 5.4540\n","Epoch 8/15\n","20/20 [==============================] - 110s 6s/step - loss: 0.3539 - binary_crossentropy: 6.1957 - val_loss: 0.7957 - val_binary_crossentropy: 5.0464\n","Epoch 8: early stopping\n","5/5 [==============================] - 5s 652ms/step\n","150/150 [==============================] - 128s 842ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 3262.89 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 7214.77 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tfxlm_roberta_for_sequence_classification_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFXLMRobertaMainL  multiple                  277453056 \n"," ayer)                                                           \n","                                                                 \n"," classifier (TFXLMRobertaCl  multiple                  592130    \n"," assificationHead)                                               \n","                                                                 \n","=================================================================\n","Total params: 278045186 (1.04 GB)\n","Trainable params: 278045186 (1.04 GB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 113s 5s/step - loss: 0.6951 - binary_crossentropy: 1.9991 - val_loss: 0.6920 - val_binary_crossentropy: 1.2151\n","Epoch 2/15\n","20/20 [==============================] - 98s 5s/step - loss: 0.6880 - binary_crossentropy: 1.8277 - val_loss: 0.6885 - val_binary_crossentropy: 1.2724\n","Epoch 3/15\n","20/20 [==============================] - 98s 5s/step - loss: 0.6808 - binary_crossentropy: 2.5086 - val_loss: 0.6479 - val_binary_crossentropy: 2.3605\n","Epoch 4/15\n","20/20 [==============================] - 97s 5s/step - loss: 0.6130 - binary_crossentropy: 3.8564 - val_loss: 0.6451 - val_binary_crossentropy: 3.7354\n","Epoch 5/15\n","20/20 [==============================] - 100s 5s/step - loss: 0.5926 - binary_crossentropy: 4.4782 - val_loss: 0.6152 - val_binary_crossentropy: 5.1710\n","Epoch 6/15\n","20/20 [==============================] - 85s 4s/step - loss: 0.5300 - binary_crossentropy: 5.5054 - val_loss: 0.5771 - val_binary_crossentropy: 5.6823\n","Epoch 7/15\n","20/20 [==============================] - 97s 5s/step - loss: 0.4771 - binary_crossentropy: 5.1223 - val_loss: 0.7266 - val_binary_crossentropy: 5.1773\n","Epoch 8/15\n","20/20 [==============================] - 98s 5s/step - loss: 0.4772 - binary_crossentropy: 5.2772 - val_loss: 0.5883 - val_binary_crossentropy: 4.8403\n","Epoch 9/15\n","20/20 [==============================] - 97s 5s/step - loss: 0.3577 - binary_crossentropy: 5.2506 - val_loss: 0.5901 - val_binary_crossentropy: 5.2849\n","Epoch 9: early stopping\n","5/5 [==============================] - 4s 462ms/step\n","150/150 [==============================] - 105s 693ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 2576.36 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 6276.55 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tfxlm_roberta_for_sequence_classification_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFXLMRobertaMainL  multiple                  277453056 \n"," ayer)                                                           \n","                                                                 \n"," classifier (TFXLMRobertaCl  multiple                  592130    \n"," assificationHead)                                               \n","                                                                 \n","=================================================================\n","Total params: 278045186 (1.04 GB)\n","Trainable params: 278045186 (1.04 GB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 102s 5s/step - loss: 0.6937 - binary_crossentropy: 7.1181 - val_loss: 0.6852 - val_binary_crossentropy: 8.0981\n","Epoch 2/15\n","20/20 [==============================] - 88s 4s/step - loss: 0.6888 - binary_crossentropy: 7.1884 - val_loss: 0.6849 - val_binary_crossentropy: 8.0981\n","Epoch 3/15\n","20/20 [==============================] - 89s 4s/step - loss: 0.6719 - binary_crossentropy: 6.9457 - val_loss: 0.6336 - val_binary_crossentropy: 6.9102\n","Epoch 4/15\n","20/20 [==============================] - 91s 5s/step - loss: 0.6475 - binary_crossentropy: 6.4858 - val_loss: 0.6423 - val_binary_crossentropy: 7.9496\n","Epoch 5/15\n","20/20 [==============================] - 90s 4s/step - loss: 0.5970 - binary_crossentropy: 5.2754 - val_loss: 0.6056 - val_binary_crossentropy: 3.9240\n","Epoch 6/15\n","20/20 [==============================] - 85s 4s/step - loss: 0.5822 - binary_crossentropy: 5.9676 - val_loss: 0.5973 - val_binary_crossentropy: 8.9099\n","Epoch 7/15\n","20/20 [==============================] - 90s 5s/step - loss: 0.5707 - binary_crossentropy: 6.6418 - val_loss: 0.5334 - val_binary_crossentropy: 7.9648\n","Epoch 8/15\n","20/20 [==============================] - 87s 4s/step - loss: 0.5036 - binary_crossentropy: 5.7186 - val_loss: 0.6510 - val_binary_crossentropy: 6.4919\n","Epoch 9/15\n","20/20 [==============================] - 90s 4s/step - loss: 0.4625 - binary_crossentropy: 5.6808 - val_loss: 0.6688 - val_binary_crossentropy: 6.7881\n","Epoch 10/15\n","20/20 [==============================] - 91s 5s/step - loss: 0.4731 - binary_crossentropy: 5.7644 - val_loss: 0.6345 - val_binary_crossentropy: 6.5035\n","Epoch 10: early stopping\n","5/5 [==============================] - 4s 576ms/step\n","150/150 [==============================] - 94s 609ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 4358.60 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 5954.22 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tfxlm_roberta_for_sequence_classification_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFXLMRobertaMainL  multiple                  277453056 \n"," ayer)                                                           \n","                                                                 \n"," classifier (TFXLMRobertaCl  multiple                  592130    \n"," assificationHead)                                               \n","                                                                 \n","=================================================================\n","Total params: 278045186 (1.04 GB)\n","Trainable params: 278045186 (1.04 GB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 91s 4s/step - loss: 0.6889 - binary_crossentropy: 2.3745 - val_loss: 0.7181 - val_binary_crossentropy: 0.8977\n","Epoch 2/15\n","20/20 [==============================] - 79s 4s/step - loss: 0.6888 - binary_crossentropy: 2.3860 - val_loss: 0.7156 - val_binary_crossentropy: 0.9166\n","Epoch 3/15\n","20/20 [==============================] - 75s 4s/step - loss: 0.6915 - binary_crossentropy: 3.0894 - val_loss: 0.7254 - val_binary_crossentropy: 4.6879\n","Epoch 4/15\n","20/20 [==============================] - 78s 4s/step - loss: 0.6956 - binary_crossentropy: 6.7103 - val_loss: 0.6985 - val_binary_crossentropy: 4.7492\n","Epoch 5/15\n","20/20 [==============================] - 81s 4s/step - loss: 0.6823 - binary_crossentropy: 5.7253 - val_loss: 0.7055 - val_binary_crossentropy: 4.7032\n","Epoch 6/15\n","20/20 [==============================] - 82s 4s/step - loss: 0.6703 - binary_crossentropy: 6.2564 - val_loss: 0.6617 - val_binary_crossentropy: 5.2260\n","Epoch 7/15\n","20/20 [==============================] - 89s 4s/step - loss: 0.6413 - binary_crossentropy: 5.5407 - val_loss: 0.7260 - val_binary_crossentropy: 4.0170\n","Epoch 8/15\n","20/20 [==============================] - 85s 4s/step - loss: 0.6275 - binary_crossentropy: 5.5615 - val_loss: 0.6451 - val_binary_crossentropy: 3.3873\n","Epoch 9/15\n","20/20 [==============================] - 84s 4s/step - loss: 0.5874 - binary_crossentropy: 5.5269 - val_loss: 0.6701 - val_binary_crossentropy: 3.3849\n","Epoch 10/15\n","20/20 [==============================] - 84s 4s/step - loss: 0.5784 - binary_crossentropy: 4.9877 - val_loss: 0.6779 - val_binary_crossentropy: 3.3193\n","Epoch 11/15\n","20/20 [==============================] - 84s 4s/step - loss: 0.5870 - binary_crossentropy: 5.0552 - val_loss: 0.6779 - val_binary_crossentropy: 3.3193\n","Epoch 11: early stopping\n","5/5 [==============================] - 5s 752ms/step\n","150/150 [==============================] - 92s 601ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 3641.34 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 7477.81 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","All PyTorch model weights were used when initializing TFXLMRobertaForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFXLMRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tfxlm_roberta_for_sequence_classification_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," roberta (TFXLMRobertaMainL  multiple                  277453056 \n"," ayer)                                                           \n","                                                                 \n"," classifier (TFXLMRobertaCl  multiple                  592130    \n"," assificationHead)                                               \n","                                                                 \n","=================================================================\n","Total params: 278045186 (1.04 GB)\n","Trainable params: 278045186 (1.04 GB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 90s 4s/step - loss: 0.6964 - binary_crossentropy: 5.4767 - val_loss: 0.7067 - val_binary_crossentropy: 6.2018\n","Epoch 2/15\n","20/20 [==============================] - 86s 4s/step - loss: 0.6870 - binary_crossentropy: 5.0311 - val_loss: 0.6680 - val_binary_crossentropy: 6.9504\n","Epoch 3/15\n","20/20 [==============================] - 85s 4s/step - loss: 0.6603 - binary_crossentropy: 5.2693 - val_loss: 0.6278 - val_binary_crossentropy: 6.1439\n","Epoch 4/15\n","20/20 [==============================] - 85s 4s/step - loss: 0.6287 - binary_crossentropy: 5.1329 - val_loss: 0.6426 - val_binary_crossentropy: 5.7896\n","Epoch 5/15\n","20/20 [==============================] - 83s 4s/step - loss: 0.5940 - binary_crossentropy: 4.4036 - val_loss: 0.6618 - val_binary_crossentropy: 5.6123\n","Epoch 6/15\n","20/20 [==============================] - 85s 4s/step - loss: 0.5712 - binary_crossentropy: 4.4622 - val_loss: 0.5162 - val_binary_crossentropy: 4.9209\n","Epoch 7/15\n","20/20 [==============================] - 86s 4s/step - loss: 0.5220 - binary_crossentropy: 4.6068 - val_loss: 0.7513 - val_binary_crossentropy: 5.1121\n","Epoch 8/15\n","20/20 [==============================] - 75s 4s/step - loss: 0.4817 - binary_crossentropy: 4.3932 - val_loss: 0.6158 - val_binary_crossentropy: 4.9861\n","Epoch 9/15\n","20/20 [==============================] - 77s 4s/step - loss: 0.4220 - binary_crossentropy: 4.4740 - val_loss: 0.5743 - val_binary_crossentropy: 5.1282\n","Epoch 9: early stopping\n","5/5 [==============================] - 4s 549ms/step\n","150/150 [==============================] - 85s 561ms/step\n"]}],"source":["fold_path = \"./../inp/gujju/folds/2023/\"\n","dirs = os.listdir(fold_path)\n","\n","oof_preds = np.zeros((data.shape[0],))\n","test_preds = np.zeros((test_data.shape[0], 2))\n","\n","for dir_name in dirs:\n","    dir_path = os.path.join(fold_path, dir_name)\n","\n","    # Defining the Train and Val paths \n","    train_df = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n","    val_df = pd.read_csv(os.path.join(dir_path, 'val.csv'))\n","    \n","    # Cleaning and Prepareing the Data\n","    train_clean = data_clean(train_df)\n","    val_clean = data_clean(val_df)\n","    \n","    # Converting to HuggingFace Datasets\n","    train_ds = Dataset.from_pandas(train_df)\n","    val_ds = Dataset.from_pandas(val_df)\n","    \n","    # Tokenize the Data    \n","    train_tokenized = train_ds.map(tokenize_examples, batched=True)\n","    val_tokenized = val_ds.map(tokenize_examples, batched=True)\n","    \n","    # Defining the Parameters for Training\n","    batch_size = 8\n","    num_epochs = 10\n","    batches_per_epoch = len(train_tokenized) // batch_size\n","    total_train_steps = int(batches_per_epoch * num_epochs)\n","    optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n","    \n","    # Define the Model \n","    model = create_model(model_name, optimizer)\n","    \n","    # Converting to Tf Dataset for training\n","    train_set = model.prepare_tf_dataset(\n","        train_tokenized,\n","        shuffle=True,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","\n","    validation_set = model.prepare_tf_dataset(\n","        val_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    test_set = model.prepare_tf_dataset(\n","        test_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    # Define Model\n","    es = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        min_delta=0,\n","        patience=3,\n","        verbose=1,\n","        mode='auto',\n","        baseline=None,\n","        restore_best_weights=False,\n","        start_from_epoch=0\n","    )\n","    \n","    history = model.fit(x=train_set,\n","              validation_data=validation_set,\n","              epochs=15,\n","              callbacks=[es])\n","\n","    logits = model.predict(validation_set).logits\n","    y_preds = tf.argmax(tf.nn.sigmoid(logits), axis=1).numpy()\n","    oof_preds[val_df['index'].values] += y_preds\n","\n","    # predict on test\n","    test_logits = model.predict(test_set).logits\n","    test_y_preds = tf.nn.sigmoid(test_logits)\n","    test_preds += test_y_preds/5\n","    "]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["np.save(\"./../out/gujurati_xmlr_preds.npy\", test_preds.numpy())"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.64      0.68      0.66       100\n","           1       0.66      0.62      0.64       100\n","\n","    accuracy                           0.65       200\n","   macro avg       0.65      0.65      0.65       200\n","weighted avg       0.65      0.65      0.65       200\n","\n"]}],"source":["y_true = data['label'].map(label2id)\n","print(classification_report(y_true, oof_preds))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","NOT    726\n","HOF    470\n","Name: count, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y_preds_test = tf.argmax(test_preds, axis=1).numpy()\n","test_data['label'] = y_preds_test\n","test_data['label'] = test_data['label'].map(id2label)\n","test_data['label'].value_counts()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023_test_main_tweet_1</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023_test_main_tweet_2</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023_test_main_tweet_3</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023_test_main_tweet_4</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023_test_main_tweet_5</td>\n","      <td>NOT</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       id label\n","0  2023_test_main_tweet_1   NOT\n","1  2023_test_main_tweet_2   HOF\n","2  2023_test_main_tweet_3   HOF\n","3  2023_test_main_tweet_4   NOT\n","4  2023_test_main_tweet_5   NOT"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["test_data['id'] = test_id\n","test_data[['id', 'label']].head()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["test_data[['id', 'label']].to_csv('./../out/guj_bert_baseline.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
