{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:54:53.110224Z","iopub.status.busy":"2023-08-25T20:54:53.109798Z","iopub.status.idle":"2023-08-25T20:55:19.953289Z","shell.execute_reply":"2023-08-25T20:55:19.952131Z","shell.execute_reply.started":"2023-08-25T20:54:53.110178Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/mrutyunjaybiswal/Documents/rcml/hasoc23-hate-speech-bengali-bodo-assamese/hasoc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import re\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","from datasets import Dataset\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorWithPadding\n","from transformers import create_optimizer\n","from transformers.keras_callbacks import KerasMetricCallback\n","from transformers import TFAutoModelForSequenceClassification\n","\n","from sklearn.metrics import classification_report, f1_score\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:19.956512Z","iopub.status.busy":"2023-08-25T20:55:19.956139Z","iopub.status.idle":"2023-08-25T20:55:20.011716Z","shell.execute_reply":"2023-08-25T20:55:20.010753Z","shell.execute_reply.started":"2023-08-25T20:55:19.956477Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv(\"./../inp/gujju/train.csv\")\n","test_data = pd.read_csv(\"./../inp/gujju/test.csv\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.085228Z","iopub.status.busy":"2023-08-25T20:55:20.084904Z","iopub.status.idle":"2023-08-25T20:55:20.089737Z","shell.execute_reply":"2023-08-25T20:55:20.088734Z","shell.execute_reply.started":"2023-08-25T20:55:20.085196Z"},"trusted":true},"outputs":[],"source":["id2label = {0: \"NOT\", 1: \"HOF\"}\n","label2id = {\"NOT\": 0, \"HOF\": 1}"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.092143Z","iopub.status.busy":"2023-08-25T20:55:20.091394Z","iopub.status.idle":"2023-08-25T20:55:20.100560Z","shell.execute_reply":"2023-08-25T20:55:20.099676Z","shell.execute_reply.started":"2023-08-25T20:55:20.092084Z"},"trusted":true},"outputs":[],"source":["def data_clean(data_df):\n","    # Removing Unwanted Columns\n","    data_df.drop([\"tweet_id\", \"created_at\", \"user_screen_name\"], axis=1, inplace=True)\n","    \n","    # Removing @tags \n","    pattern = r'@\\w+'\n","    data_df[\"text\"] = data_df[\"text\"].apply(lambda x: re.sub(pattern, '', x))\n","    \n","    # Transforming Categorical Values to Numericals\n","    data_df[\"labels\"] = data_df[\"label\"].apply(lambda x: [label2id[x]])\n","    \n","    # Dropping label column\n","    data_df.drop(\"label\", axis=1, inplace=True)\n","    \n","    return data_df"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.104445Z","iopub.status.busy":"2023-08-25T20:55:20.104144Z","iopub.status.idle":"2023-08-25T20:55:20.123435Z","shell.execute_reply":"2023-08-25T20:55:20.122458Z","shell.execute_reply.started":"2023-08-25T20:55:20.104416Z"},"trusted":true},"outputs":[],"source":["# Cleaning and Preparing Test Data\n","test_id = test_data[\"tweet_id\"]\n","test_data.drop(\"tweet_id\", axis=1, inplace=True)\n","pattern = r'@\\w+'\n","test_data[\"text\"] = test_data[\"text\"].apply(lambda x: re.sub(pattern, '', x))"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:20.125659Z","iopub.status.busy":"2023-08-25T20:55:20.125221Z","iopub.status.idle":"2023-08-25T20:55:21.347551Z","shell.execute_reply":"2023-08-25T20:55:21.346508Z","shell.execute_reply.started":"2023-08-25T20:55:20.125573Z"},"trusted":true},"outputs":[],"source":["model_name = \"l3cube-pune/gujarati-bert\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, from_pt=True)\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.349436Z","iopub.status.busy":"2023-08-25T20:55:21.348943Z","iopub.status.idle":"2023-08-25T20:55:21.866335Z","shell.execute_reply":"2023-08-25T20:55:21.865190Z","shell.execute_reply.started":"2023-08-25T20:55:21.349391Z"},"trusted":true},"outputs":[],"source":["def tokenize_examples(examples):\n","    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, max_length=True)\n","    return tokenized_inputs\n","\n","def create_model(model_name, optimizer):\n","    model = TFAutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels=len(label2id),\n","        id2label=id2label,\n","        label2id=label2id,\n","        from_pt=True\n","    )\n","    \n","    model.compile(\n","        optimizer=optimizer,\n","        metrics=[tf.keras.metrics.binary_crossentropy]\n","    )\n","    \n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:21.868477Z","iopub.status.busy":"2023-08-25T20:55:21.868074Z","iopub.status.idle":"2023-08-25T20:55:22.196953Z","shell.execute_reply":"2023-08-25T20:55:22.195959Z","shell.execute_reply.started":"2023-08-25T20:55:21.868440Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 1196/1196 [00:00<00:00, 9570.37 examples/s]\n"]}],"source":["test = Dataset.from_pandas(test_data)\n","test_tokenized = test.map(tokenize_examples, batched=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-25T20:55:22.199049Z","iopub.status.busy":"2023-08-25T20:55:22.198674Z","iopub.status.idle":"2023-08-25T20:58:30.257943Z","shell.execute_reply":"2023-08-25T20:58:30.256422Z","shell.execute_reply.started":"2023-08-25T20:55:22.199013Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 22512.95 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 13605.72 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_37 (Dropout)        multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/15\n","20/20 [==============================] - 58s 3s/step - loss: 0.6927 - binary_crossentropy: 6.8421 - val_loss: 0.6925 - val_binary_crossentropy: 7.3523\n","Epoch 2/15\n","20/20 [==============================] - 52s 3s/step - loss: 0.6895 - binary_crossentropy: 6.2945 - val_loss: 0.6891 - val_binary_crossentropy: 5.1383\n","Epoch 3/15\n","20/20 [==============================] - 54s 3s/step - loss: 0.6771 - binary_crossentropy: 5.0680 - val_loss: 0.6789 - val_binary_crossentropy: 4.8278\n","Epoch 4/15\n","20/20 [==============================] - 64s 3s/step - loss: 0.6497 - binary_crossentropy: 4.6508 - val_loss: 0.6671 - val_binary_crossentropy: 4.6362\n","Epoch 5/15\n","20/20 [==============================] - 62s 3s/step - loss: 0.6094 - binary_crossentropy: 4.4512 - val_loss: 0.6475 - val_binary_crossentropy: 4.4076\n","Epoch 6/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.5622 - binary_crossentropy: 4.3460 - val_loss: 0.6188 - val_binary_crossentropy: 4.3275\n","Epoch 7/15\n","20/20 [==============================] - 76s 4s/step - loss: 0.5245 - binary_crossentropy: 4.2929 - val_loss: 0.6196 - val_binary_crossentropy: 4.2899\n","Epoch 8/15\n","20/20 [==============================] - 72s 4s/step - loss: 0.5023 - binary_crossentropy: 4.2768 - val_loss: 0.6481 - val_binary_crossentropy: 4.2650\n","Epoch 9/15\n","20/20 [==============================] - 77s 4s/step - loss: 0.4876 - binary_crossentropy: 4.2539 - val_loss: 0.6291 - val_binary_crossentropy: 4.2541\n","Epoch 9: early stopping\n","5/5 [==============================] - 4s 592ms/step\n","150/150 [==============================] - 87s 575ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 4617.62 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 7902.23 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_75 (Dropout)        multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 82s 4s/step - loss: 0.6927 - binary_crossentropy: 5.2431 - val_loss: 0.6933 - val_binary_crossentropy: 5.7300\n","Epoch 2/15\n","20/20 [==============================] - 71s 4s/step - loss: 0.6906 - binary_crossentropy: 5.0138 - val_loss: 0.6916 - val_binary_crossentropy: 5.6118\n","Epoch 3/15\n","20/20 [==============================] - 85s 4s/step - loss: 0.6801 - binary_crossentropy: 4.8850 - val_loss: 0.6865 - val_binary_crossentropy: 5.5252\n","Epoch 4/15\n","20/20 [==============================] - 68s 3s/step - loss: 0.6589 - binary_crossentropy: 4.9952 - val_loss: 0.6835 - val_binary_crossentropy: 6.8532\n","Epoch 5/15\n","20/20 [==============================] - 63s 3s/step - loss: 0.6366 - binary_crossentropy: 5.7749 - val_loss: 0.6753 - val_binary_crossentropy: 5.5626\n","Epoch 6/15\n","20/20 [==============================] - 63s 3s/step - loss: 0.6205 - binary_crossentropy: 4.9996 - val_loss: 0.6634 - val_binary_crossentropy: 5.3406\n","Epoch 7/15\n","20/20 [==============================] - 57s 3s/step - loss: 0.6047 - binary_crossentropy: 4.8001 - val_loss: 0.6788 - val_binary_crossentropy: 5.2442\n","Epoch 8/15\n","20/20 [==============================] - 57s 3s/step - loss: 0.5871 - binary_crossentropy: 4.7256 - val_loss: 0.6726 - val_binary_crossentropy: 5.1850\n","Epoch 9/15\n","20/20 [==============================] - 56s 3s/step - loss: 0.5779 - binary_crossentropy: 4.6923 - val_loss: 0.6753 - val_binary_crossentropy: 5.1326\n","Epoch 9: early stopping\n","5/5 [==============================] - 3s 369ms/step\n","150/150 [==============================] - 75s 491ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 4844.81 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 5781.06 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_113 (Dropout)       multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 62s 3s/step - loss: 0.6933 - binary_crossentropy: 4.0135 - val_loss: 0.6926 - val_binary_crossentropy: 4.1003\n","Epoch 2/15\n","20/20 [==============================] - 59s 3s/step - loss: 0.6916 - binary_crossentropy: 3.9052 - val_loss: 0.6905 - val_binary_crossentropy: 4.4128\n","Epoch 3/15\n","20/20 [==============================] - 61s 3s/step - loss: 0.6850 - binary_crossentropy: 4.1149 - val_loss: 0.6881 - val_binary_crossentropy: 4.5105\n","Epoch 4/15\n","20/20 [==============================] - 63s 3s/step - loss: 0.6713 - binary_crossentropy: 3.7188 - val_loss: 0.6832 - val_binary_crossentropy: 4.0609\n","Epoch 5/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.6528 - binary_crossentropy: 3.7597 - val_loss: 0.6835 - val_binary_crossentropy: 4.5559\n","Epoch 6/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.6320 - binary_crossentropy: 4.6323 - val_loss: 0.6794 - val_binary_crossentropy: 4.8522\n","Epoch 7/15\n","20/20 [==============================] - 58s 3s/step - loss: 0.6131 - binary_crossentropy: 4.6321 - val_loss: 0.6705 - val_binary_crossentropy: 4.7697\n","Epoch 8/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.6022 - binary_crossentropy: 4.7127 - val_loss: 0.6666 - val_binary_crossentropy: 4.8129\n","Epoch 9/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.5898 - binary_crossentropy: 4.6851 - val_loss: 0.6637 - val_binary_crossentropy: 4.8132\n","Epoch 10/15\n","20/20 [==============================] - 59s 3s/step - loss: 0.5859 - binary_crossentropy: 4.6617 - val_loss: 0.6664 - val_binary_crossentropy: 4.8190\n","Epoch 11/15\n","20/20 [==============================] - 58s 3s/step - loss: 0.5850 - binary_crossentropy: 4.6559 - val_loss: 0.6664 - val_binary_crossentropy: 4.8190\n","Epoch 12/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.5872 - binary_crossentropy: 4.6583 - val_loss: 0.6664 - val_binary_crossentropy: 4.8190\n","Epoch 12: early stopping\n","5/5 [==============================] - 4s 518ms/step\n","150/150 [==============================] - 77s 504ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 7021.89 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 7662.93 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_151 (Dropout)       multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 63s 3s/step - loss: 0.6928 - binary_crossentropy: 5.0989 - val_loss: 0.6953 - val_binary_crossentropy: 3.5267\n","Epoch 2/15\n","20/20 [==============================] - 61s 3s/step - loss: 0.6893 - binary_crossentropy: 5.2237 - val_loss: 0.7024 - val_binary_crossentropy: 3.3806\n","Epoch 3/15\n","20/20 [==============================] - 58s 3s/step - loss: 0.6773 - binary_crossentropy: 4.9751 - val_loss: 0.7123 - val_binary_crossentropy: 3.3436\n","Epoch 4/15\n","20/20 [==============================] - 58s 3s/step - loss: 0.6552 - binary_crossentropy: 4.8263 - val_loss: 0.7130 - val_binary_crossentropy: 3.3586\n","Epoch 4: early stopping\n","5/5 [==============================] - 4s 567ms/step\n","150/150 [==============================] - 75s 494ms/step\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 160/160 [00:00<00:00, 11932.16 examples/s]\n","Map: 100%|██████████| 40/40 [00:00<00:00, 7619.43 examples/s]\n","WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"tf_bert_for_sequence_classification_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  237556224 \n","                                                                 \n"," dropout_189 (Dropout)       multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 237557762 (906.21 MB)\n","Trainable params: 237557762 (906.21 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","Epoch 1/15\n","20/20 [==============================] - 66s 3s/step - loss: 0.6932 - binary_crossentropy: 4.2798 - val_loss: 0.6934 - val_binary_crossentropy: 5.9468\n","Epoch 2/15\n","20/20 [==============================] - 61s 3s/step - loss: 0.6926 - binary_crossentropy: 4.8315 - val_loss: 0.6925 - val_binary_crossentropy: 5.8139\n","Epoch 3/15\n","20/20 [==============================] - 61s 3s/step - loss: 0.6890 - binary_crossentropy: 4.7068 - val_loss: 0.6863 - val_binary_crossentropy: 5.4838\n","Epoch 4/15\n","20/20 [==============================] - 62s 3s/step - loss: 0.6744 - binary_crossentropy: 4.4861 - val_loss: 0.6716 - val_binary_crossentropy: 5.3279\n","Epoch 5/15\n","20/20 [==============================] - 60s 3s/step - loss: 0.6423 - binary_crossentropy: 4.4341 - val_loss: 0.6566 - val_binary_crossentropy: 5.0637\n","Epoch 6/15\n","20/20 [==============================] - 61s 3s/step - loss: 0.6034 - binary_crossentropy: 4.2735 - val_loss: 0.6373 - val_binary_crossentropy: 5.0811\n","Epoch 7/15\n","20/20 [==============================] - 61s 3s/step - loss: 0.5642 - binary_crossentropy: 4.2311 - val_loss: 0.6477 - val_binary_crossentropy: 5.0910\n","Epoch 8/15\n","20/20 [==============================] - 62s 3s/step - loss: 0.5375 - binary_crossentropy: 4.1987 - val_loss: 0.6349 - val_binary_crossentropy: 4.9821\n","Epoch 9/15\n","20/20 [==============================] - 62s 3s/step - loss: 0.5135 - binary_crossentropy: 4.1620 - val_loss: 0.6296 - val_binary_crossentropy: 4.9505\n","Epoch 10/15\n","20/20 [==============================] - 58s 3s/step - loss: 0.5071 - binary_crossentropy: 4.1626 - val_loss: 0.6304 - val_binary_crossentropy: 4.9662\n","Epoch 11/15\n","20/20 [==============================] - 61s 3s/step - loss: 0.5066 - binary_crossentropy: 4.1597 - val_loss: 0.6304 - val_binary_crossentropy: 4.9662\n","Epoch 12/15\n","20/20 [==============================] - 62s 3s/step - loss: 0.5014 - binary_crossentropy: 4.2027 - val_loss: 0.6304 - val_binary_crossentropy: 4.9662\n","Epoch 12: early stopping\n","5/5 [==============================] - 4s 512ms/step\n","150/150 [==============================] - 76s 498ms/step\n"]}],"source":["fold_path = \"./../inp/gujju/folds/2023/\"\n","dirs = os.listdir(fold_path)\n","\n","oof_preds = np.zeros((data.shape[0],))\n","test_preds = np.zeros((test_data.shape[0], 2))\n","\n","for dir_name in dirs:\n","    dir_path = os.path.join(fold_path, dir_name)\n","\n","    # Defining the Train and Val paths \n","    train_df = pd.read_csv(os.path.join(dir_path, 'train.csv'))\n","    val_df = pd.read_csv(os.path.join(dir_path, 'val.csv'))\n","    \n","    # Cleaning and Prepareing the Data\n","    train_clean = data_clean(train_df)\n","    val_clean = data_clean(val_df)\n","    \n","    # Converting to HuggingFace Datasets\n","    train_ds = Dataset.from_pandas(train_df)\n","    val_ds = Dataset.from_pandas(val_df)\n","    \n","    # Tokenize the Data    \n","    train_tokenized = train_ds.map(tokenize_examples, batched=True)\n","    val_tokenized = val_ds.map(tokenize_examples, batched=True)\n","    \n","    # Defining the Parameters for Training\n","    batch_size = 8\n","    num_epochs = 10\n","    batches_per_epoch = len(train_tokenized) // batch_size\n","    total_train_steps = int(batches_per_epoch * num_epochs)\n","    optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n","    \n","    # Define the Model \n","    model = create_model(model_name, optimizer)\n","    \n","    # Converting to Tf Dataset for training\n","    train_set = model.prepare_tf_dataset(\n","        train_tokenized,\n","        shuffle=True,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","\n","    validation_set = model.prepare_tf_dataset(\n","        val_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    test_set = model.prepare_tf_dataset(\n","        test_tokenized,\n","        shuffle=False,\n","        batch_size=8,\n","        collate_fn=data_collator,\n","    )\n","    \n","    # Define Model\n","    es = tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        min_delta=0,\n","        patience=3,\n","        verbose=1,\n","        mode='auto',\n","        baseline=None,\n","        restore_best_weights=False,\n","        start_from_epoch=0\n","    )\n","    \n","    history = model.fit(x=train_set,\n","              validation_data=validation_set,\n","              epochs=15,\n","              callbacks=[es])\n","\n","    logits = model.predict(validation_set).logits\n","    y_preds = tf.argmax(tf.nn.sigmoid(logits), axis=1).numpy()\n","    oof_preds[val_df['index'].values] += y_preds\n","\n","    # predict on test\n","    test_logits = model.predict(test_set).logits\n","    test_y_preds = tf.nn.sigmoid(test_logits)\n","    test_preds += test_y_preds/5\n","    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.68      0.53      0.60       100\n","           1       0.61      0.75      0.68       100\n","\n","    accuracy                           0.64       200\n","   macro avg       0.65      0.64      0.64       200\n","weighted avg       0.65      0.64      0.64       200\n","\n"]}],"source":["y_true = data['label'].map(label2id)\n","print(classification_report(y_true, oof_preds))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["label\n","NOT    736\n","HOF    460\n","Name: count, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["y_preds_test = tf.argmax(test_preds, axis=1).numpy()\n","test_data['label'] = y_preds_test\n","test_data['label'] = test_data['label'].map(id2label)\n","test_data['label'].value_counts()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2023_test_main_tweet_1</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2023_test_main_tweet_2</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2023_test_main_tweet_3</td>\n","      <td>HOF</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2023_test_main_tweet_4</td>\n","      <td>NOT</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2023_test_main_tweet_5</td>\n","      <td>HOF</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       id label\n","0  2023_test_main_tweet_1   NOT\n","1  2023_test_main_tweet_2   HOF\n","2  2023_test_main_tweet_3   HOF\n","3  2023_test_main_tweet_4   NOT\n","4  2023_test_main_tweet_5   HOF"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["test_data['id'] = test_id\n","test_data[['id', 'label']].head()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["test_data[['id', 'label']].to_csv('./../out/guj_test_baseline.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
